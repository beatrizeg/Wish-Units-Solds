---
title: "Product sales prediction"
author: "Beatriz Estrella"
date: "12/10/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = FALSE, message = FALSE, warning = FALSE)
options(tinytex.verbose = TRUE)
```

# 1. INTRODUCTION
## 1.1. Project goal

The motivation of this project is to complete the task of the course in Data Science: Capstone from HarvardX, course PH125.9x, under the *Choose your own!* project submission.

The project aims to predict the units sold for each specific product. The database was downloaded from - [Kaggle](https://www.kaggle.com/jmmvutu/summer-products-and-sales-in-ecommerce-wish) and consists of data from platform [wish](https://www.wish.com). *Wish* is a retailer that sells millions of product to customer in a e-commerce marketplace. The data consists of the products under the tag "summer" showed during August 2020.

We want to see which features can predict the units that will be sold in a specific product, and the accuracy that we get under the different machine learning algorithms. 

This is a multinomial classification problem as units sold will be treated as a categorical value, due to the low number of different values we have for this column in the dataset. That way, we will use different Machine Learning algorithms that can predict under this assumption.

Apart from the main dataset (stored as data frame in main variable), we have a support dataset that contains the main tag categories and the count on how many times they are used in the platform, thus we can deduce that the higher the count, the more relevant they are (stored as data frame in cat variable).

Datasets, .R code and all relevant documentation regarding this project can be found online at [github.com/beatrizeg/Wish-Units-Solds](https://github.com/beatrizeg/Wish-Units-Solds).

```{r load Data and libraries, echo=FALSE}
library(tidyverse)
library(stringr)
library(purrr)
library(caret)
library(ggplot2)
library(corrplot)
library(forcats)
library(rattle)
library(xgboost)
library(klaR)

url_main <- "https://raw.githubusercontent.com/beatrizeg/Wish-Units-Solds/main/summer-products-with-rating-and-performance_2020-08.csv"
dest_file <- "data/main.csv"
download.file(url_main, destfile = dest_file)
main <- read_csv("data/main.csv")
save(main, file = "rdas/main.rda")

url_cat <- "https://raw.githubusercontent.com/beatrizeg/Wish-Units-Solds/main/unique-categories.sorted-by-count.csv"
dest_file_cat <- "data/cat.csv"
download.file(url_cat, destfile = dest_file_cat)
cat <- read_csv("data/cat.csv")
save(cat, file = "rdas/cat.rda")

load("rdas/main.rda")
load("rdas/cat.rda")

main <- as.data.frame(main)
cat <- as.data.frame(cat)
```

## 1.2. Inspecting the dataset

Now we proceed to inspect the main dataset that is directly downloaded from Kaggle. Using the dim and summary functions we see:

```{r inspect}
dim(main)
summary(main)
```

The dataset consists of 1573 rows and 43 columns. We see as well that we have character and numeric values and that we also have some NAs in the dataset. 

First thing we have to take into account now, is that having only ~1k of data points is not a lot. This will probably lead to the accuracy of the prediction not being too high. We will be able to check on this later on.

To check which columns gives us NAs we use. 

```{r nas}
apply(main, 2, function(x) any(is.na(x)))
```

So we proceed to substitute the NAs in *rating_<X>_count* columns to 0 and also the ones in the *has_urgency_banner* to 0 as 0 represents *FALSE*.

We see that we still get NAs in columns *product_color*, *product_variation_size_id*, *urgency_text*, *origin_country*, *merchant_name*, *merchant_info_subtitle* and *merchant_profile_picture*, but we will deal with these later as we will see, because these features will not be included in the model, except for *product_color*, *product_variation_size_id* and *merchant_profile_picture* that will be tidied up later.

Lastly, it is important to note that the *product_id* is the unique differentiator of each product. We will see later the reason for a few of them being duplicated and solve the issue.

## 1.3. Libraries

All of these these libraries will be loaded:

```{r libraries, echo=TRUE, eval=FALSE}
library(stringr)
library(purrr)
library(caret)
library(ggplot2)
library(corrplot)
library(forcats)
library(rattle)
library(xgboost)
library(klaR)
library(h2o)
```

# 2. METHOD AND ANALYSIS
## 2.1. Exploration of the dataset

Now we proceed to inspect the different features/variables in the dataset, taking into account that the value we want to be able to predict is *units_sold*.

### 2.1.1. Checking features variability and adjusting
#### *product_color*

First of all we are going to study the different categories under the predictor of *product_color*. We can see all the different colors we get by showing the table and histogram below. 

```{r product_color}
table(main$product_color) %>% sort(decreasing = TRUE)
main %>% ggplot(aes(product_color))+geom_bar()
```

We see that most of the categories only apply for 2 or less *products_id*, so we will group into main color categories this feature in order to provide the algorithm with more valuable data.

```{r color class}
main <- main %>% mutate(product_color=
                          as.factor(case_when(
                            str_detect(product_color, "&") ~ "two colors",
                            str_detect(product_color, "blue") ~ "blue",
                            str_detect(product_color, "navy") ~ "blue",
                            str_detect(product_color, "green") ~ "green",
                            str_detect(product_color, "red") ~ "red",
                            str_detect(product_color, "gray") ~ "grey",
                            str_detect(product_color, "grey") ~ "grey",
                            str_detect(product_color, "coffee") ~ "brown",
                            str_detect(product_color, "brown") ~ "brown",
                            str_detect(product_color, "pink") ~ "pink",
                            str_detect(product_color, "rose") ~ "pink",
                            str_detect(product_color, "black") ~ "black",
                            str_detect(product_color, "white") ~ "white",
                            str_detect(product_color, "purple") ~ "purple",
                            str_detect(product_color, "orange") ~ "orange",
                            str_detect(product_color, "multicolor") ~ "multicolor",
                            str_detect(product_color, "yellow") ~ "yellow",
                            TRUE ~ "other")))

main %>% ggplot(aes(product_color))+geom_bar()
```

Now we get just a few main categories which allows as to treat this feature as a categorical value.

#### *product_variation_size_id*

We are going to do the same exercise we did with the *product_color* variable with the *product_variation_size_id*. We can check the high variability and low information it provides as it is given in the dataset.

```{r product_size}
table(main$product_variation_size_id) %>% sort(decreasing = TRUE)
main %>% ggplot(aes(product_variation_size_id))+geom_bar()
```

We again reassign the values to the main categories and again, we treat it as a categorical value, which provides much more information to the algorithms. 

```{r size reassign, echo=FALSE}
main <- main %>% mutate(product_variation_size_id=
                          as.factor(case_when(product_variation_size_id=="XXXS" ~ "XXXS",
                                   product_variation_size_id=="XXS" ~ "XXS",
                                   product_variation_size_id=="XS" | 
                                     product_variation_size_id=="XS." |
                                     product_variation_size_id=="SIZE XS" |
                                     product_variation_size_id=="Size-XS" |
                                     product_variation_size_id=="Size-XS" ~ "XS",
                                   product_variation_size_id=="S" | 
                                     product_variation_size_id=="S." |
                                     product_variation_size_id=="s" |
                                     product_variation_size_id=="Size S" |
                                     product_variation_size_id=="Size-S" |
                                     product_variation_size_id=="size S" |
                                     product_variation_size_id=="Size S." |
                                     product_variation_size_id=="S Pink" |
                                     product_variation_size_id=="Suit-S"~ "XS",
                                   product_variation_size_id=="M" | 
                                     product_variation_size_id=="M."~ "M",
                                   product_variation_size_id=="L" | 
                                     product_variation_size_id=="SizeL" ~ "L",
                                   product_variation_size_id=="XL"   ~ "XL",
                                   product_variation_size_id=="XXL" | 
                                     product_variation_size_id=="2XL" ~ "XXL",
                                   product_variation_size_id=="XXXL" ~ "XXXL",
                                   product_variation_size_id=="4XL" ~ "4XL",
                                   TRUE ~ "other")))
```
```{r size graph}
table(main$product_variation_size_id) %>% sort(decreasing = TRUE)
main %>% ggplot(aes(product_variation_size_id))+geom_bar()
```

#### *origin_country*

We again see variability for *origin_country*, and reassign converting the variable to factor.

```{r ocountry, echo=FALSE, eval=TRUE}
table(main$origin_country) %>% sort(decreasing = TRUE)
main <- main %>% mutate(
  origin_country=as.factor(case_when(
    origin_country == "CN" | origin_country == "US" ~ origin_country,
    TRUE ~ "other"
  )))

main %>% ggplot(aes(origin_country))+geom_bar()
```

#### *currency_buyer*

We check that there is only one currency in the dataset and that no unification of units is needed.

```{r currency}
n_distinct(main$currency_buyer)
```

#### *units_sold*

Now we study the characteristics of the variable we want to predict.

```{r sold}
table(main$units_sold) %>% sort(decreasing = TRUE)
```

We see that there are only 15 different values, and in fact six of them are below 10 so we could group this into 9 different categories and treat the project as a categorical problem. This is what we will do.

```{r sold graphs, echo=FALSE}
main <- main %>% mutate(units_sold = ifelse(units_sold<10, 10, units_sold))
main %>% ggplot(aes(factor(units_sold)))+geom_bar()
```

#### *product_id*

As we commented in the introduction, we can easily check that there are less unique *product_id* values than rows in the dataset. In fact, there are 1341 different *product_id* and 1573 rows.

```{r duplicates}
n_distinct(main$product_id)
```

So we are going to examine an example of a duplicated *product_id* (ex. "5577faf03cef83230c39d0c3") and see the differences in the rows.

```{r exam dup}
main %>% group_by(product_id) %>% summarize(n=n()) %>% arrange(desc(n))
main %>% filter(product_id=="5577faf03cef83230c39d0c3") %>% dplyr::select(-merchant_name)
```

We can see that the almost every column acquires the same value, except for *has_urgency_banner*. Thus we can easily deduce that during the month, this feature was changed for the *product_id* and a new row was created. As the impact is minimal and we do not know which of the rows was active during most of the month, we will just simply delete the duplicated rows as:

```{r remove dup}
main <- distinct(main, product_id, .keep_all = TRUE)
```

### 2.1.2. Assigning classes to features and calculating % stars rating instead of total count

Now we have studied the variability of the features that had a class "character" and reassigned them the class "factor" for the algorithms to work better, a few more predictors that acquire few different values are assigned to factor as well. We also saw that some features acquired either a 0 or a 1, so we will change this to logical class. 

For the columns of rating_X_count, instead of keeping the total count, we will keep the percentage by dividing for each * by the total counts.

```{r classes}
main <- main %>% mutate(currency_buyer=as.factor(currency_buyer),
                        badges_count=as.factor(badges_count),
                        uses_ad_boosts=as.logical(uses_ad_boosts),
                        badge_local_product=as.logical(badge_local_product),
                        badge_product_quality=as.logical(badge_product_quality),
                        badge_fast_shipping=as.logical(badge_fast_shipping),
                        shipping_option_price=as.factor(shipping_option_price),
                        shipping_is_express=as.logical(shipping_is_express),
                        has_urgency_banner=as.logical(has_urgency_banner),
                        merchant_has_profile_picture=as.logical(merchant_has_profile_picture),
                        inventory_total=as.factor(inventory_total))

main <- main %>% mutate(rating_five_count=rating_five_count/rating_count,
                        rating_four_count=rating_four_count/rating_count,
                        rating_three_count=rating_three_count/rating_count,
                        rating_two_count=rating_two_count/rating_count,
                        rating_one_count=rating_one_count/rating_count)

main <- main %>% mutate(rating_five_count=ifelse(is.na(rating_five_count),0,rating_five_count),
                        rating_four_count=ifelse(is.na(rating_four_count),0,rating_four_count),
                        rating_three_count=ifelse(is.na(rating_three_count),0,rating_three_count),
                        rating_two_count=ifelse(is.na(rating_two_count),0,rating_two_count),
                        rating_one_count=ifelse(is.na(rating_one_count),0,rating_one_count))
```

### 2.1.3. Introducing tags model

In the main dataset, we can see a column named *tags* that includes all tags that were given to each *product_id*. In the cat dataset, there are two columns, the column *counts* gives the number of times each tag, which is in column *keyword*, appears in the main dataset. We can assume then, that those keywords that appear the most, are more relevant than those than appear the less. Thus we create a new column in the cat dataset, named *cat_n*, with a number that goes from 1 to 4, as in code below. What we are trying to do here is to have a column that provides a relevance number for the tags that were used in the *product_id*. 

```{r cat_n}
cat <- cat %>% mutate(cat_n =
                        case_when(count>=1000 ~ 4,
                                  count<1000 & count>=500 ~ 3,
                                  count<500 & count>=200 ~ 2,
                                  count < 200 ~ 1,
                                  TRUE ~ 0))
```

Once we have this, we want to transfer this information to the main dataset. First, the *tags* column contains all tags separated by a comma. We split all the tags into different columns, getting 41 columns. Then, we substitute each tag for its value *cat_n* that we assigned in the cat dataset. Once we have the values in *numeric* class, we sum all tag values for each row or *product_id* and bind this new column, named *n_tags* to the main dataset.

```{r n_tags}
main_tags <- str_split(main$tags, ",", simplify = TRUE)

for (i in 1:41){
main_tags[,i] <- with(cat, cat_n[match(main_tags[,i], keyword)])
} #next step change to numeric values 

main_tags <- as.data.frame(main_tags)
main_tags[] <- lapply(main_tags, function(x) as.numeric(as.character(x))) 

main_tags <- main_tags %>% mutate(n_tags = rowSums(main_tags, na.rm=TRUE)) %>% dplyr::select(n_tags)

main_m <- bind_cols(main, main_tags)
```

Lastly, we disregards all 41 columns that were created and keep only those that can provide relevant information *(price, retail_price, units_sold, uses_ad_boosts, rating, rating_count,  rating_five_count, rating_four_count, rating_three_count, rating_two_count, rating_one_count, badges_count, badge_local_product, badge_product_quality, badge_fast_shipping, product_color, product_variation_size_id, product_variation_inventory, shipping_option_price, shipping_is_express, countries_shipped_to, inventory_total, has_urgency_banner, origin_country, merchant_rating_count, merchant_rating, merchant_has_profile_picture, product_id, n_tags)*, disregarding also those such as *title*, *merchant_name*, *product_url*, etc.

```{r select, echo=FALSE}
main_m <- main_m %>% dplyr::select(price, retail_price, units_sold, uses_ad_boosts, rating, rating_count, 
                 rating_five_count, rating_four_count, rating_three_count, rating_two_count, rating_one_count,
                 badges_count, badge_local_product, badge_product_quality, badge_fast_shipping,
                 product_color, product_variation_size_id, product_variation_inventory,
                 shipping_option_price, shipping_is_express, countries_shipped_to, inventory_total,
                 has_urgency_banner, origin_country, merchant_rating_count, merchant_rating,
                 merchant_has_profile_picture, product_id, n_tags)
```

### 2.1.4. Predictors that do not vary across sample

We are going to do a quick check on predictors that maintain close to the same value across all the sample to evaluate if we want to disregard them. We will use:

```{r no_var}
no_var <- nearZeroVar(main_m, saveMetrics = TRUE)
no_var[no_var[,"zeroVar"] + no_var[,"nzv"] > 0, ] 
```

As we can see, there are no predictors with zero variance, but there are 5 predictors with near zero variance. For those five, only *inventory_total* is unique for the 74.5% of the *product_id*, the rest have higher variance. As we are not sure how having a low value in this column might impact the units sold, we decide to still keep all of them.

### 2.1.5. Adding *perc_price* column

From the dataset we see that we have a *price* column, and a *retail_price* column. *Price* is the price for which the product is being sold in the platform, and *retail_price* is the price that similar products have in the same or in other platforms, we can say, a benchmark price. To include any possible sales effect, we add a new column that measures if the price is higher or lower than average.

```{r perc_price}
main_m <- main_m %>% mutate(perc_price=(price-retail_price)/retail_price)
```

### 2.1.6. Studying correlation between variables

To study correlation between variables we are going to do two different plots. First, we will do the correlation matrix for the numeric variables.

```{r cor}
main_m.cor <- main_m %>% mutate(units_sold=as.numeric(units_sold)) %>%
  dplyr::select_if(is.numeric) %>%
  cor(.)
corrplot(main_m.cor)
```

We can see that the units sold are highly affected by the total number of ratings, thus by the count of ratings for every star, but we cannot see a high relation for more units sold with a higher rating or higher % of five star ratings. As well, there is a correlation with the number of ratings for the merchant and its rating. There is also a slight positive correlation with the *product_variation_inventory* variable.

To study correlation between non numerical variables, we will perform a Chi-squared test of independence, looking at the p-values. We can say that two different variables are independent if the probability distribution of one is not affected by the presence of the other.

In this case, the null hypotheses is that the variables are independent from the units sold. With a significance level of 0.05, we will test the hypotheses of them being independent.

```{r chisq}
main_m.chisq <- main_m %>%
  dplyr::select_if(function(col) is.character(col) | 
              is.factor(col) | is.logical(col) |
              all(col == .$units_sold)) %>% dplyr::select(-product_id)

columns <- 1:ncol(main_m.chisq)
vars <- names(main_m.chisq)[columns]
out <-  apply( combn(columns,2),2,function(x){
  chisq.test(table(main_m.chisq[,x[1]],main_m.chisq[,x[2]]),correct=F)$p.value
})

out <- cbind(as.data.frame(t(combn(vars,2))),out)
out_dep <- out %>% filter(V1=="units_sold") %>% filter(out<0.05) %>%arrange(out)
out_dep
```

We can see that we get p-values below 0.05 for 6 variables, thus, we can reject in these cases the null hypothesis and assume that these are not independent to units_sold. 

Also, we can assume that these 7 other predictors are independent and do not affect the output of the units sold. We will be able to check on this when we study the variable importance of the machine learning algorithms.

```{r independent}
out_ind <- out %>% filter(V1=="units_sold") %>% filter(out>=0.05) %>% arrange(out)
out_ind
```

### 2.2. Checking predictors effect - graphs

In this section we will show different graphs with the relationship between units sold and other predictors.

**product_color**

```{r color graph, echo=FALSE}
main_m %>% 
ggplot(aes(fct_infreq(product_color), units_sold)) + geom_bar(stat = "identity") +
  ggtitle("Product Color") + xlab("product_color")
```

**product_size_id**

```{r size graph, echo=FALSE}
main_m %>% 
  ggplot(aes(fct_infreq(product_variation_size_id), units_sold)) + geom_bar(stat = "identity") +
   ggtitle("Product Size") + xlab("product_size_id")
```

**price**

```{r price graph, echo=FALSE}
main_m %>% 
  ggplot(aes(price, units_sold)) + geom_smooth() +
  ggtitle("Price") + xlab("price")
```

**perc_price**

```{r perc_price graph, echo=FALSE}
main_m %>% 
  ggplot(aes(perc_price, units_sold)) + geom_smooth() +
  ggtitle("Percentage price") + xlab("perc_price")
```

**uses_ad_boosts**

```{r ad_boosts graph, echo=FALSE}
main_m %>% 
  ggplot(aes(uses_ad_boosts, as.numeric(units_sold))) + geom_bar(stat="identity") +
  ggtitle("Uses Ad boosts") + ylab("units_sold")
```

**5 star %**

```{r 5star graph, echo=FALSE}
main_m %>% 
  ggplot(aes(rating_five_count, units_sold)) + geom_smooth() +
  ggtitle("Percentage of 5 stars") + xlab("perc 5*")
```

**1 star %**

```{r 5star graph, echo=FALSE}
main_m %>% 
  ggplot(aes(rating_one_count, units_sold)) + geom_smooth() +
  ggtitle("Percentage of 1 star") + xlab("perc 1*")
```

Lastly, as we commented before, we change the class of *units_sold* to factor, so that it is categorical.

```{r factor}
levels <- c("10", "50", "100", "1000", "5000", "10000", "20000", "50000", "1e+05")
main_p <- main_m %>% mutate(units_sold = factor(units_sold, levels=levels))
```

## 2.3. Creation of train and test set

We split our data into a train set where we run the algorithms, and a test set to test the results and see how good our algorithm did. Test set will have 15% of the data and train set 85%. Our train set consists of 1138 rows and test set of 203 rows.

```{r train test}
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(main_m$units_sold, times=1, p=0.15, list=FALSE)
train_set <- main_p[-test_index,] %>% dplyr::select(-product_id)
test_set <- main_p[test_index,] %>% dplyr::select(-product_id) 
```

## 2.4. Machine Learning algorithms

### 2.4.1. GAM Loess

Gam Loess is a Generalized Additive Model that keeps the inspiration of Linear Models but incorporating non-linear forms. The model uses smooth functions of the predictors, and the loess function is used to fit. The loess function acts as a local regression as fitting at point x is weighted toward the data closest to x. The data considered from x is controlled by the *span* argument, and represents the percentage of data that will be taken into account.

To optimize the *span* argument, we will use repeated cross validation. 

Repeated Cross-Validation has two different values, *number* and *repeats*. *Number* means the times we ramdonly divide our data into, so in this case, we will created 3 different sets to test results while training the model in the other 2. *Repeats* means that we will repeat 4 times the process for each partition, and it will calcuate the average. 

We only divide into 3 different sets due to the small amount of data we have.

```{r train gam}
set.seed(1, sample.kind = "Rounding")
control <- trainControl(method = "repeatedcv", number = 3, repeats = 4, savePredictions = "all")
grid_loess <- expand.grid(span=seq(0.2,0.9,0.2), degree=1)
train_loess <- train(units_sold ~ ., data=train_set, method="gamLoess", trControl=control, tuneGrid=grid_loess)
train_loess
ggplot(train_loess, highlight = TRUE)
```

So best *span* in this case is 0.6, and we only get an accuracy of 0.05139. If we tried guessing, the probability of being correct would be 1/9 = 0.11. So we would do better just by guessing! It even gets worse when predicting in the test_set, as we can see in the table below.

```{r test gam}
y_loess <- predict(train_loess, test_set, type="raw")
acc_loess <- confusionMatrix(y_loess, test_set$units_sold)$overall[['Accuracy']]
acc_results <- tibble(method = "Gam Loess", Accuracy_Train = max(train_loess$results$Accuracy), Accuracy_Test = acc_loess)
acc_results
```

### 2.4.2. K nearest neighbors

Now we will use the method of k-nearest neighbors. This method relies on the assumption that similar features will provide the same output. The model equals "similar features" to closeness in distance.

In this case, we need to optimize the *k* argument, which indicates the number of neighbors to include in every calculation. We use as well repeated cross-validation to find the optimal *k*.

```{r train knn}
set.seed(2007, sample.kind = "Rounding")
control <- trainControl(method = "repeatedcv", number=3, repeats=4)
train_knn <- train(units_sold ~ ., data=train_set, method="knn", tuneGrid = data.frame(k=seq(3, 40, 2)), trControl=control)
ggplot(train_knn, highlight = TRUE)
```

In this case, we get the higher accuracy with a k=5. In that case, the accuracy in the train_set is 0.5074. Now we are doing better than guessing. Let's check our results in the test_set. 

```{r test knn}
y_knn <- predict(train_knn, test_set, type="raw")
acc_knn <- confusionMatrix(y_knn, test_set$units_sold)$overall[['Accuracy']]
acc_results <- bind_rows(acc_results,
                         data_frame(method="KNN", Accuracy_Train = max(train_knn$results$Accuracy),
                                    Accuracy_Test = acc_knn))
acc_results
```

It gets even better, now we predict correctly more than half of the units sold.

### 2.4.3. Neural networks

In this section, we introduce a Machine Learning algorithm that we did not study during the course, but that is very powerful and easy to understand. It is inspired in replicating the way of working of the neurons in the brain.  

neural network: ![](neural.png)

Using the *caret* package, we can optimize now two different parameters: *size* and *decay*. *Size* is the number of units in hidden layer and *decay* is a parameter of regularization to avoid over-fitting that can vary between 0 (default) and 1.

We will try with sizes going from 4 to 20 and decays from 0.05 to 0.5, and a few minutes later we get the results.

```{r train nnet1}
set.seed(2007, sample.kind = "Rounding")
control <- trainControl(method = "repeatedcv", number=3, repeats=4)
grid_nnet1 <- expand.grid(size=seq(4,20,4), decay=seq(0.05, 0.5, 0.02))
train_nnet1 <- train(units_sold ~ ., data=train_set, method="nnet", trControl=control, tuneGrid=grid_nnet1)
ggplot(train_nnet1, highlight = TRUE)
```

We see that the best results are for size=8 and decay=0.49, which provides an accuracy on train_set of 0.58193. Looks like we are improving!

Now we will run again the code but focusing on sizes from 4 to 8 and decays from 0.4 to 0.6 to see if we can get a higher accuracy.

```{r train nnet2}
set.seed(2007, sample.kind = "Rounding")
control <- trainControl(method = "repeatedcv", number=3, repeats=4)
grid_nnet2 <- expand.grid(size=seq(4,8,2), decay=seq(0.4, 0.6, 0.02))
train_nnet2 <- train(units_sold ~ ., data=train_set, method="nnet", trControl=control, tuneGrid=grid_nnet2)
ggplot(train_nnet2, highlight = TRUE)
```

And the best results are for size=6 and decay=0.6, providing an accuracy of 0.58782. So now, we will use this train_nnet2 model to predict the results in the test set. 

```{r test nnet2}
y_nnet <- predict(train_nnet2, test_set, type="raw")
acc_nnet <- confusionMatrix(y_nnet, test_set$units_sold)$overall[['Accuracy']]
acc_results <- bind_rows(acc_results,
                         data_frame(method="Neural Network", Accuracy_Train = max(train_nnet2$results$Accuracy),
                                    Accuracy_Test = acc_nnet))
acc_results
```

And we see an improvement as well! Now we are over 0.59 of accuracy on the test set.

### 2.4.4. Classification Trees

In this section we will use the classification trees model. This is a very intuitive model that is highly used for categorical problems. 

The idea is to create a decision tree, with cut points so that, when applied in a hierarchical way, lead to the final output.

*Caret* package allows us to optimize the *cp* parameter via tuneGrid. *cp* is the complexity parameter of the tree and its default is 0. It helps decide the depth of the tree by not continuing building the tree if the improvement does not meet the criteria.

Firstly, we will use default values to train the model, with regular cross-validation for computer optimization purposes. Also we need to change the levels name of the *units_sold* parameter for the function to not throw and error message.

```{r train default rpart}
levels(train_set$units_sold) <- c("X10", "X50", "X100", "X1000", "X5000", "X10000", "X20000", "X50000", "X05")
levels(test_set$units_sold) <- c("X10", "X50", "X100", "X1000", "X5000", "X10000", "X20000", "X50000", "X05")

set.seed(2007, sample.kind = "Rounding")
control <- trainControl(method = "cv", number=4, classProbs = TRUE)
train_rpart0 <- train(units_sold ~ ., data=train_set, method="rpart", trControl=control)
ggplot(train_rpart0, highlight = TRUE)
```

We see that the optimal value for cp in this case is for cp=0.07196, which provides an accuracy of 0.61248.
To visualize the tree, we will use the fancyRpartPlot from {rattle} library. We can see that it is actually quite simple, and that only one feature, *rating_count*, is used to make the predictions! Also, we can see the variable importance. 

```{r train rpart0 tree}
fancyRpartPlot(train_rpart0$finalModel, sub = NULL)
train_rpart0$finalModel$variable.importance
```

For the purpose of predicting using the model and be able to compare results, we will use it to predict and get accuracy on test set.

```{r test rpart0 tree, echo=FALSE}
y_rpart0 <- predict(train_rpart0, test_set, type="raw")
acc_rpart0 <- confusionMatrix(y_rpart0, test_set$units_sold)$overall[['Accuracy']]
acc_results <- bind_rows(acc_results,
                         data_frame(method="Regression Trees not optimised", Accuracy_Train = max(train_rpart0$results$Accuracy),
                                    Accuracy_Test = acc_rpart0))
acc_results
```

Now we will use tuneGrid to try different values of cp that range from 0 to 0.05, as it seems to be the area with the higher accuracy values. *Minsplit* will be held constant at 15. 

```{r train rpart1 tree}
set.seed(2007, sample.kind = "Rounding")
control1 <- trainControl(method = "cv", number=4, classProbs = TRUE)
train_rpart1 <- train(units_sold ~ ., data=train_set, method="rpart", tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)), control=rpart::rpart.control(minsplit=15), trControl=control1)
ggplot(train_rpart1, highlight = TRUE)
```

We now get a higher accuracy of 0.68542 with a cp=0.010416, mainly due to the variation of the *minsplit* parameter. Now that we have optimized cp, we will try different minsplit numbers to optimize the model. 

```{r train minsplit tree}
cp <- train_rpart1$bestTune$cp
minsplit <- seq(10, 40, len=8)
acc <- sapply(minsplit, function(ms){
  train(units_sold ~ ., method = "rpart", data = train_set, tuneGrid = data.frame(cp=cp),
        control=rpart::rpart.control(minsplit=ms))$results$Accuracy })
qplot(minsplit, acc)
minsplit[which.max(acc)]
max(acc)
minsplit <- minsplit[which.max(acc)]
```

Great, for a minsplit=18.57 of we get an accuracy of 0.6906. Let's now train a new model with the optimized cp and minsplit values and plot the new tree.

We see now that the tree gets more decision points, but still is suprising, only the feature *rating_count* is taken into consideration. 

```{r train opt tree}
train_rpart2 <- train(units_sold ~ ., data=train_set, method="rpart", tuneGrid = data.frame(cp = cp), control=rpart::rpart.control(minsplit=minsplit), trControl=control1)
fancyRpartPlot(train_rpart2$finalModel, sub = NULL)
train_rpart2$finalModel$variable.importance
```

The accuracy we get with this model in the train set is 0.68895, higher than in the previous tree ones, so this is the one we will use for predictions.

```{r test opt tree}
y_rpart2 <- predict(train_rpart2, test_set, type="raw")
acc_rpart2 <- confusionMatrix(y_rpart2, test_set$units_sold)$overall[['Accuracy']]
acc_results <- bind_rows(acc_results,
                         data_frame(method="Regression Trees Optimized", Accuracy_Train = max(train_rpart2$results$Accuracy),
                                    Accuracy_Test = acc_rpart2))
```

### 2.4.5. Random Forest
